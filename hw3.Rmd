---
title: 'Homework 3: Databases, web scraping, and a basic Shiny app'
author: "Andrea Villarreal"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

```{r warning = FALSE}
#| label: Loading libraries
#| message: false
#| warning: false

library(tidyverse)
library(wbstats)
library(tictoc)
library(skimr)
library(countrycode)
library(here)
library(DBI)
library(dbplyr)
library(arrow)
library(rvest)
library(robotstxt) # check if we're allowed to scrape the data
library(scales)
library(sf)
library(readxl)
library(robotstxt)

```

# Money in UK politics

[The Westminster Accounts](https://news.sky.com/story/the-westminster-accounts-12786091), a recent collaboration between Sky News and Tortoise Media, examines the flow of money through UK politics. It does so by combining data from three key sources: 

1. [Register of Members’ Financial Interests](https://www.parliament.uk/mps-lords-and-offices/standards-and-financial-interests/parliamentary-commissioner-for-standards/registers-of-interests/register-of-members-financial-interests/), 
1. [Electoral Commission records of donations to parties](http://search.electoralcommission.org.uk/English/Search/Donations), and
1. [Register of All-Party Parliamentary Groups](https://www.parliament.uk/mps-lords-and-offices/standards-and-financial-interests/parliamentary-commissioner-for-standards/registers-of-interests/register-of-all-party-party-parliamentary-groups/). 


## Open a connection to the database

The database made available by Simon Willison is an `SQLite` database

```{r}
sky_westminster <- DBI::dbConnect(
  drv = RSQLite::SQLite(),
  dbname = here::here("data", "sky-westminster-files.db")
)
```

```{r}
# Sky Westinster tables
DBI::dbListTables(sky_westminster)
```

## Which MP has received the most amount of money? 

```{r}
# Fetching the 'payments' and 'members' tables from the database
payments <- dbReadTable(sky_westminster, 'payments')
members <- dbReadTable(sky_westminster, 'members')
```

```{r}
# Summarizing the donations by member ID (which corresponds to the same ID as in the members dataframe) and then sorting the data to find the MP who raised the most
mp_receipts <- payments %>%
  group_by(member_id) %>%
  summarise(donations = round(sum(value))) %>%
  rename(id = member_id) %>%
  left_join(members, by = 'id') %>%
  arrange(desc(donations))

print(mp_receipts[0:5, c('name', 'donations')])
```

```{r}
print(paste0 ('The MP who received the most amount of money was ', mp_receipts$name[1], ' with £', mp_receipts$donations[1]))
```


## Any `entity` that accounts for more than 5% of all donations?

```{r}
# Total donations received
total_donations <- round(sum(payments$value))

# Summarizing by entity to aggregate the donations they've made across the entire dataset
entity_donations <- payments %>%
  group_by(entity) %>%
  summarize(donations = round(sum(value))) %>%
  mutate(percent = round(donations / total_donations, 4)) %>%
  arrange(desc(donations))

# Entities who accounted for more than 5% of total payments given to MPs
entity_5p <- entity_donations %>%
  filter(percent > .05)

print(entity_5p)
# Withers LLP accounted for 5.25% of total donations
```


## Do `entity` donors give to a single party or not?

- How many distinct entities who paid money to MPS are there?
- How many (as a number and %) donated to MPs belonging to a single party only?

```{r}
# Number of distinct entities who paid money to MPs - used the n_distinct function
entities <- unique(payments$entity)
num_entities <- n_distinct(payments$entity)

print(paste0('The number of distinct entities is ', num_entities))
```

```{r}
# To figure out to which parties the entities invested in, we join the two databases to match the party id with the entity donation
party_donations <- payments %>%
  group_by(entity, member_id) %>%
  summarise(donations = round(sum(value))) %>%
  rename(id = member_id) %>%
  left_join(members, by = 'id') %>%
  select(-c(gender, status, short_name))

# Dataframe to summarize entity donations by type of party
entity_parties <- party_donations %>%
  group_by(entity, party_id) %>%
  summarise(donations = n()) %>%
  group_by(entity) %>%
  summarise(parties = n(),
            donations = sum(donations))
```

```{r}
# Filter the entities who donated to a single party
entity_single <- nrow(filter(entity_parties, parties == 1))

print(paste0('The number of entities who donated to a single party are ', entity_single, ' which is ', round(entity_single/num_entities*100), '% of the total'))
```


## Which party has raised the greatest amount of money in each of the years 2020-2022? 


```{r}
parties <- dbReadTable(sky_westminster, 'parties')
donations <- dbReadTable(sky_westminster, 'party_donations')
```

```{r}
# Joining multiple datasets to include the relevant information
party_receipts <- parties %>%
  select(id, name) %>%
  rename(party_id = id) %>%
  left_join(donations, by = 'party_id') %>%
  group_by(date, name) %>%
  summarise(total_year_donations = sum(value)) %>%
  rename(year = date) %>%
  mutate(prop = total_year_donations/total_donations)

# Changing the date to keep only the year
party_receipts$year <- as.numeric(format(as.Date(party_receipts$year), '%Y'))

party_receipts <- party_receipts %>%
  group_by(year, name) %>%
  summarize(total_year_donations = sum(total_year_donations),
            prop = sum(prop)) %>%
  drop_na() 
```
```{r}
party_receipts
```


```{r}
party_receipts <- party_receipts %>%
  group_by(year) %>%
  arrange(desc(total_year_donations), .by_group = TRUE) %>%
  mutate(name = as.factor(name),
         year = as.factor(year))

# Plotting the data on a graph
donations_graph <- ggplot(party_receipts, aes(x = year, y = total_year_donations, fill = reorder(name, desc(total_year_donations)))) +
  geom_col(position = 'dodge') +
  labs(title = 'Conservatives have captured the majority of political donations', 
       subtitle = 'Donations to political parties, 2020-2022',
       x = element_blank(),
       y = element_blank()) +
  guides(fill = guide_legend(title = 'Party')) +
  theme_light()
donations_graph
```


```{r}
# Disconnecting 
dbDisconnect(sky_westminster)
```


# Anonymised Covid patient data from the CDC

We will be using a dataset with [anonymous Covid-19 patient data that the CDC publishes every month](https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data-with-Ge/n8mc-b4w4). The file we will use was released on April 11, 2023, and has data on 98 million of patients, with 19 features. This file cannot be loaded in memory, but luckily we have the data in `parquet` format and we will use the `{arrow}` package.

## Obtain the data

```{r}
#| echo: false
#| message: false
#| warning: false


tic() # start timer
cdc_data <- open_dataset(here::here("data", "cdc-covid-geography"))
toc() # stop timer

glimpse(cdc_data)
```


```{r}
# Collecting necessary data from the dataset
covid_data <- cdc_data %>%
  group_by(age_group, sex, icu_yn, death_yn) %>%
  summarise(count = n()) %>%
  collect()
```

```{r}
# Excluding NAs and missing information
covid_data <- covid_data %>%
  filter(sex == 'Female' | sex == 'Male') %>%
  filter(icu_yn == 'No' | icu_yn == 'Yes') %>%
  filter(death_yn == 'No' | death_yn == 'Yes') %>%
  filter(age_group != 'Missing')

# Calculating % on the patient categories
covid_total <- covid_data %>%
  group_by(sex, age_group, icu_yn) %>%
  summarize(sum = sum(count))

# Combining the data by using a left join on multiple criteria
covid_perc <- covid_data %>%
  left_join(covid_total, by = c('sex', 'age_group', 'icu_yn')) %>%
  mutate(perc = case_when(death_yn == 'Yes' ~ round(count / sum * 100),
                          death_yn == 'No' ~ 0)) %>%
  mutate(icu_yn = case_when(icu_yn == 'Yes' ~ 'ICU Admission',
                            icu_yn == 'No' ~ 'No ICU Admission'))
  

```

```{r}

# Plotting the data
covid_plot <- ggplot(covid_perc) +
  geom_col(aes(x = age_group, y = perc, fill = 'coral')) +
  coord_flip() +
  facet_grid(rows = vars(icu_yn), cols = vars(sex)) +
  geom_text(aes(x = age_group, y = perc, label = ifelse(death_yn == 'Yes', perc, '')), hjust = 1, size = 3) +
  labs(title = 'Covid CFR % by age group, sex and ICU admission',
       y = element_blank(),
       x = element_blank()) +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  guides(fill = 'none') +
  theme_light()

covid_plot
```

The previous plot is an aggregate plot for all three years of data. What if we wanted to plot Case Fatality Ratio (CFR) over time? 


```{r}
# Collecting necessary data from the dataset, including case_month variable in the extraction
covid_data_time <- cdc_data %>%
  group_by(age_group, sex, icu_yn, death_yn, case_month) %>%
  summarise(count = n()) %>%
  collect()
```

```{r}
# Excluding NAs and missing information
covid_data_time <- covid_data_time %>%
  filter(sex == 'Female' | sex == 'Male') %>%
  filter(icu_yn == 'No' | icu_yn == 'Yes') %>%
  filter(death_yn == 'No' | death_yn == 'Yes') %>%
  filter(age_group != 'Missing')

# Calculating % on the patient categories
covid_total_time <- covid_data_time %>%
  group_by(sex, age_group, icu_yn, case_month) %>%
  summarize(sum = sum(count))

# Combining the data by using a left join on multiple criteria
covid_perc_time <- covid_data_time %>%
  left_join(covid_total_time, by = c('sex', 'age_group', 'icu_yn', 'case_month')) %>%
  mutate(perc = case_when(death_yn == 'Yes' ~ round(count / sum * 100),
                          death_yn == 'No' ~ 0)) %>%
  mutate(icu_yn = case_when(icu_yn == 'Yes' ~ 'ICU Admission',
                            icu_yn == 'No' ~ 'No ICU Admission'))
```

```{r}
# Excluding non-mortality cases
covid_perc_plot <- covid_perc_time %>%
  filter(death_yn == 'Yes') %>%
  filter(case_month != '2020-02') # Takes away the first month which screws up the data trend

# Plotting the data
covid_time_plot <- ggplot(covid_perc_plot, aes(x = case_month, y = perc, color = age_group, group = age_group)) +
  geom_point() + geom_line() +
  facet_grid(rows = vars(icu_yn), cols = vars(sex), scales = 'free') +
  geom_text(aes(x = case_month, y = perc, label = ifelse(death_yn == 'Yes', perc, '')), hjust = -0.3, size = 3) +
  labs(title = 'Covid CFR % by age group, sex and ICU admission',
       y = element_blank(),
       x = element_blank()) +
  scale_color_discrete(name = 'Age Group') +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, size = 5))
covid_time_plot
```



For each patient, the dataframe also lists the patient's states and county [FIPS code](https://en.wikipedia.org/wiki/Federal_Information_Processing_Standard_state_code). The CDC also has information on the [NCHS Urban-Rural classification scheme for counties](https://www.cdc.gov/nchs/data_access/urban_rural.htm)

```{r}
urban_rural <- read_xlsx(here::here("data", "NCHSURCodes2013.xlsx")) %>% 
  janitor::clean_names() 

urban_rural <- urban_rural %>%
  rename(county_fips_code = fips_code)
```


Each county belongs in six different categories, with categories 1-4 being urban areas and categories 5-6 being rural, according to the following criteria captured in `x2013_code`

Category name

1. Large central metro - 1 million or more population and contains the entire population of the largest principal city
2. large fringe metro - 1 million or more poulation, but does not qualify as 1
3. Medium metro - 250K - 1 million population
4. Small metropolitan population < 250K
5. Micropolitan 
6. Noncore

```{r}
# Collecting necessary data from the dataset, which is now un-grouped
# Data frame collects all fatalities
patient_fatalities <- cdc_data %>%
  filter(death_yn == 'Yes') %>%
  filter(case_month != '2020-01' & case_month != '2020-02') %>%
  group_by(case_month, county_fips_code) %>%
  summarise(fatalities = n()) %>%
  collect()

# Data frame collects all cases
patient_data <- cdc_data %>%
  filter(case_month != '2020-01' & case_month != '2020-02') %>%
  group_by(case_month, county_fips_code) %>%
  summarise(cases = n()) %>%
  collect()

```


```{r}
# Joining patient data with county data via the county code
patient_county <- patient_data %>%
  left_join(patient_fatalities, by = c('case_month', 'county_fips_code')) %>%
  left_join(urban_rural, by = 'county_fips_code') %>%
  select(-c(state_abr, cbsa_title)) %>%
  drop_na() %>%
  mutate(x2013_code = case_when(x2013_code == 1 ~ 'Large central metro',
                                x2013_code == 2 ~ 'Large fringe metro',
                                x2013_code == 3 ~ 'Medium metro',
                                x2013_code == 4 ~ 'Small metropolitan',
                                x2013_code == 5 ~ 'Micropolitan',
                                x2013_code == 6 ~ 'Noncore')) %>%
  select(-c(x2006_code, x1990_based_code)) %>%
  mutate(county = as.factor(x2013_code)) 

```


```{r}
# Calculated the CFR by dividing fatalities by total number of cases by county and then aggregating over types of counties. I realize there are other ways of calculating this, but this seemed most natural
patient_cfr <- patient_county %>%
  group_by(case_month, county) %>%
  summarise(cases = sum(cases),
            fatalities = sum(fatalities)) %>%
  drop_na() %>%
  mutate(cfr = round(fatalities / cases * 100)) %>%
  filter(cfr != 100)
```

```{r}
# Plotting the data
county_plot <- ggplot(patient_cfr, aes(x = case_month, y = cfr, color = county, group = county)) +
  geom_point() + geom_line() +
  facet_wrap(~factor(county, levels = c('Large central metro', 'Large fringe metro', 'Medium metro', 'Small metropolitan', 'Micropolitan', 'Noncore')), nrow = 3, scales = 'free') +
  labs(title = 'Covid CFR % by country population',
       y = element_blank(),
       x = element_blank()) +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  guides(color = 'none') +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, size = 5), panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
county_plot
```

```{r}
# Taking the data set made for the previous plot and aggregating over urban/rural
cfr_rural <- patient_county %>%
  mutate(size = case_when(county == 'Large central metro' ~ 'Urban',
                          county == 'Large fringe metro' ~ 'Urban',
                          county == 'Medium metro' ~ 'Urban',
                          county == 'Small metropolitan' ~ 'Urban',
                          county == 'Micropolitan' ~ 'Rural',
                          county == 'Noncore' ~ 'Rural')) %>%
  select(-c(cbsa_2012_pop, x2013_code, county_2012_pop, county_name)) %>%
  group_by(size, case_month) %>%
  summarise(cases = sum(cases),
            fatalities = sum(fatalities)) %>%
  mutate(cfr = round(fatalities / cases * 100)) %>%
  filter(cfr != 100)
  
```

```{r}
# Plotting the data
rural_plot <- ggplot(cfr_rural, aes(x = case_month, y = cfr, group = size, color = size)) +
  geom_point() + geom_line() +
  labs(title = 'Covid CFR % by rural and urban areas',
       y = element_blank(),
       x = element_blank()) +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) +
  scale_color_discrete(name = 'Counties') +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, size = 5), panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
rural_plot

```


# Money in US politics

In the United States, [*"only American citizens (and immigrants with green cards) can contribute to federal politics, but the American divisions of foreign companies can form political action committees (PACs) and collect contributions from their American employees."*]

We will scrape and work with data foreign connected PACs that donate to US political campaigns. The data for foreign connected PAC contributions in the 2022 election cycle can be found at https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022. Then, we will use a similar approach to get data such contributions from previous years so that we can examine trends over time.

All data come from [OpenSecrets.org](https://www.opensecrets.org), a *"website tracking the influence of money on U.S. politics, and how that money affects policy and citizens' lives"*.

```{r}
#| label: allow-scraping-opensecrets
#| warning: false
#| message: false

paths_allowed("https://www.opensecrets.org")

base_url <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"



```

```{r}
# Scraping data for 2022
contributions_tables <- base_url %>% read_html() 

# Convert to data frame
contributions <- contributions_tables %>% html_table()

# Extracting tibble from the list, using janitor::clean_names() to rename variables scraped using `snake_case` naming
campaigns <- contributions[[1]] %>%
  janitor::clean_names() # Cleaning column names
```


- Clean the data: 

```{r, eval=FALSE}
# write a function to parse_currency and convert contribution amounts from character strings to numeric values - applied to 'total', 'dems' and 'repubs'
parse_currency <- function(x){
  x %>%
    
    # remove dollar signs
    str_remove("\\$") %>%
    
    # remove all occurrences of commas
    str_remove_all(",") %>%
    
    # convert to numeric
    as.numeric()
}

# clean country/parent co and contributions 
campaigns <- campaigns %>%
  separate(country_of_origin_parent_company, # Separate such that country appears in dif column
           into = c("country", "parent"), 
           sep = "/", 
           extra = "merge") %>%
  mutate(
    total = parse_currency(total),
    dems = parse_currency(dems),
    repubs = parse_currency(repubs)
  )
```

- Creating a function that will do the above for any new URL related to foreign-connected PAC contributions from Open Secrets
    
```{r}
# Function that scrapes information from the Open Secrets webpage for foreign-connected PAC contributions

scrape_pac <- function(url) {
  # URL converted into list of tables
  scraped_data <- url %>%
    read_html() %>%
    html_table() 
  
  # Extracting the year from the URL
  yr <- str_sub(url, -4)
  
  # Table extracted and converted to a dataframe
  df <- scraped_data[[1]] %>%
    janitor::clean_names()
  
  # Function to remove currency signs
  parse_currency <- function(x){
  x %>%
    str_remove("\\$") %>%
    str_remove_all(",") %>%
    as.numeric()
  }
  
  # Cleaning data
  df <- df %>% 
    separate(country_of_origin_parent_company,
             into = c("country", "parent"),
             sep = "/",
             extra = "merge") %>%
    mutate(total = parse_currency(total),
           dems = parse_currency(dems),
           repubs = parse_currency(repubs),
           year = yr
           )
  
  return(df)
}
```


```{r}
# Contributions for 2022, testing out the function above
contributions2022 <- scrape_pac(base_url)
glimpse(contributions2022)
```

```{r}
# URLs for 2022, 2020 and 2000 contributions
url2022 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
url2020 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020"
url2000 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000"

# Vetor that combines the above
urls <- c(url2000, url2020, url2022)

```


```{r}
# The following maps over the URLs in the 'url' vector and combines the data in a final dataframe called 'contributions_all'
n <- length(urls)
data <- vector('list', length = n)

for(i in 1:n) {
  data[[i]] <- scrape_pac(urls[i])
}

contributions_all <- bind_rows(data)
```


```{r}
# Writing dataframe to csv file 
write_csv(contributions_all, file = here::here('data', 'contributions-all.csv'))
```


# Scraping consulting jobs

The website [https://www.consultancy.uk/jobs/](https://www.consultancy.uk/jobs) lists job openings for consulting jobs.

```{r}
#| label: consulting_jobs_url
#| eval: false

paths_allowed("https://www.consultancy.uk") #is it ok to scrape?

base_url2 <- "https://www.consultancy.uk/jobs/page/1"

listings_html <- base_url2 %>%
  read_html() 

listings_data <- listings_html %>%
  html_table()

listings <- listings_data[[1]] %>%
  janitor::clean_names()

```

Identify the CSS selectors in order to extract the relevant information from this page, namely

1. job 
1. firm
1. functional area
1. type

```{r}
# Function that scrapes information from the webpage for consulting jobs
scrape_jobs <- function(url) {
  # Scraping data from url and turning into tables
  listings <- url %>%
    read_html() %>%
    html_table()
  
  # Table extracted and converted to a dataframe
  df <- listings[[1]] %>%
    janitor::clean_names()
  
  return(df)
}
```


```{r}
# Testing the function on page 1 and page 2
url1 <- 'https://www.consultancy.uk/jobs/page/1'
url2 <- 'https://www.consultancy.uk/jobs/page/2'

jobs1 <- scrape_jobs(url1)
jobs2 <- scrape_jobs(url2)
```


```{r}
# Base url
url_base <- 'https://www.consultancy.uk/jobs/page/'

pages <- c('1', '2', '3', '4', '5')
p_length <- length(pages)

jobs <- vector('list', length = p_length)

for(i in 1:p_length) {
  u <- str_c(url_base, i)
  jobs[[i]] <- scrape_jobs(u)
}

all_consulting_jobs <- bind_rows(jobs)

```


```{r}
# Writing dataframe to csv file 
write_csv(all_consulting_jobs, file = here::here('data', 'all_consulting_jobs.csv'))

```






