<<<<<<< HEAD
---
title: "Homework 2"
author: "Andrea Villarreal"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---
---

```{r}
#| label: Loading Libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(wbstats)
library(skimr)
library(countrycode)
library(here)
library(ggrepel)
library(patchwork)
```

# Data Visualisation - Exploration

Now that you've demonstrated your software is setup, and you have the basics of data manipulation, the goal of this assignment is to practice transforming, visualising, and exploring data.

# Mass shootings in the US

In July 2012, in the aftermath of a mass shooting in a movie theater in Aurora, Colorado, [Mother Jones](https://www.motherjones.com/politics/2012/07/mass-shootings-map/) published a report on mass shootings in the United States since 1982. Importantly, they provided the underlying data set as [an open-source database](https://www.motherjones.com/politics/2012/12/mass-shootings-mother-jones-full-data/) for anyone interested in studying and understanding this criminal behavior.

## Obtain the data

```{r}
#| echo: false
#| message: false
#| warning: false


mass_shootings <- read_csv(here::here("data", "mass_shootings.csv"))

glimpse(mass_shootings)
```

| column(variable)     | description                                                                 |
|--------------------------|----------------------------------------------|
| case                 | short name of incident                                                      |
| year, month, day     | year, month, day in which the shooting occurred                             |
| location             | city and state where the shooting occcurred                                 |
| summary              | brief description of the incident                                           |
| fatalities           | Number of fatalities in the incident, excluding the shooter                 |
| injured              | Number of injured, non-fatal victims in the incident, excluding the shooter |
| total_victims        | number of total victims in the incident, excluding the shooter              |
| location_type        | generic location in which the shooting took place                           |
| male                 | logical value, indicating whether the shooter was male                      |
| age_of_shooter       | age of the shooter when the incident occured                                |
| race                 | race of the shooter                                                         |
| prior_mental_illness | did the shooter show evidence of mental illness prior to the incident?      |

## Explore the data

### Specific questions

-   Generate a data frame that summarizes the number of mass shootings per year.

```{r}

# Data frame groups by year and new column is created with frequency per year
yearly_shootings <- mass_shootings %>%
  group_by(year) %>%
  summarise(number = n(), fatalities = sum(fatalities), injured = sum(injured), total_victims = sum(total_victims))

# Plotting a bar chart with number of shootings each year
year_plot <- ggplot(yearly_shootings) + 
  geom_col(aes(y = number, x = year, fill = year), show.legend = FALSE) + 
  labs(x = 'Year', y = 'Number of Shootings', title = 'Number of Mass Shootings per Year') +
  scale_fill_fermenter() +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
year_plot

```

-   Generate a bar chart that identifies the number of mass shooters associated with each race category. The bars should be sorted from highest to lowest and each bar should show its number.

```{r}
# Data frame that groups by race
race_shootings <- mass_shootings %>%
  group_by(race) %>%
  summarise(sum = n()) %>%
  drop_na() %>%
  arrange(desc(sum))

# Plotting a bar chart with number of shootings per race
race_plot <- ggplot(race_shootings) +
  geom_col(aes(x = reorder(race, sum), y = sum, fill = race), show.legend = FALSE) +
  geom_text(aes(x = reorder(race, sum), y = sum, label = sum), vjust = -0.5, size = 3) +
  labs(x = 'Race', y = 'Number of Shootings', title = 'Number of Mass Shootings per Race') +
  scale_fill_brewer() +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
race_plot

```

-   Generate a boxplot visualizing the number of total victims, by type of location.

```{r}
# To visualise the number of total victims by type of location, there is no need to create a new dataframe that summarises the data

# Plotting a boxplot with the number of total victims by location type
location_plot <- ggplot(mass_shootings, aes(x = location_type, y = total_victims)) +
  geom_boxplot(aes(fill = location_type), show.legend = FALSE) + 
  labs(x = 'Location Type', y = 'Total Victims', title = 'Number of Total Victims by Location Type') +
  scale_fill_brewer(palette = 'Dark') +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
location_plot
```

-   Redraw the same plot, but remove the Las Vegas Strip massacre from the dataset.

```{r}
# To visualise the number of total victims by type of location, there is no need to create a new dataframe that summarises the data

# Plotting a boxplot with the number of total victims by location type
location_plot2 <- ggplot(mass_shootings, aes(x = location_type, y = total_victims)) +
  geom_boxplot(outlier.shape = NA, aes(fill = location_type), show.legend = FALSE) + 
  coord_cartesian(ylim = c(0, 50)) + # Taking out outliers and readjusting axes
  labs(x = 'Location Type', y = 'Total Victims', title = 'Number of Total Victims by Location Type') +
  scale_fill_brewer(palette = 'Dark') +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
location_plot2
```

### More open-ended questions

Address the following questions. Generate appropriate figures/tables to support your conclusions.

-   How many white males with prior signs of mental illness initiated a mass shooting after 2000?

```{r}
# Data frame to filter for the key criteria (race, year and gender)
white_shooters <- mass_shootings %>%
  filter(race == 'White' & year > 2000 & male == TRUE) %>%
  group_by(prior_mental_illness) %>%
  summarise(number = n())

head(white_shooters, 3)

```

-   Which month of the year has the most mass shootings? Generate a bar chart sorted in chronological (natural) order (Jan-Feb-Mar- etc) to provide evidence of your answer.

```{r}
# Data frame to summarize shootings by month
month_shootings <- mass_shootings %>%
  group_by(month) %>%
  summarise(number = n()) %>%
  mutate(month = factor(month, levels = month.abb)) %>%
  arrange(month)

# Plotting a bar graph with shootings by month
month_plot <- ggplot(month_shootings, aes(x = month, y = number)) +
  geom_col(aes(fill = month), show.legend = FALSE) + 
  labs(x = 'Month', y = 'Number of Mass Shootings', title = 'Number of Mass Shootings by Month') +
  scale_fill_brewer(palette = 'Set3') +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
month_plot
```

-   How does the distribution of mass shooting fatalities differ between White and Black shooters? What about White and Latino shooters?

```{r}
# Data frame that filters data for 'black' and 'white' shooters
bw_shootings <- mass_shootings %>%
  filter(race == 'White' | race == 'Black') %>%
  filter(total_victims < 150) # Excluding the Las Vegas shooting

# Histogram for distribution of total victims - Black vs. White
bw_hist <- ggplot(bw_shootings, aes(x = total_victims, fill = race)) +
  geom_density(alpha = 0.6) +
  labs(x = 'Total Victims', y = 'Frequency', title = 'Distribution of Total Victims of Mass Shootings') +
  guides(fill = guide_legend(title = 'Race')) +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))  
bw_hist
```

```{r}
# Data frame that filters data for 'latino' and 'white' shooters
lw_shootings <- mass_shootings %>%
  filter(race == 'White' | race == 'Latino') %>%
  filter(total_victims < 150) # Excluding the Las Vegas shooting

# Histogram for distribution of total victims - Latino vs. White
lw_hist <- ggplot(lw_shootings, aes(x = total_victims, fill = race)) +
  geom_density(alpha = 0.6) +
  labs(x = 'Total Victims', y = 'Frequency', title = 'Distribution of Total Victims of Mass Shootings') +
  guides(fill = guide_legend(title = 'Race')) +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))  
lw_hist
```

### Very open-ended

-   Are mass shootings with shooters suffering from mental illness different from mass shootings with no signs of mental illness in the shooter?

```{r}
# Data frame that filters data for mental illness (excludes NA)
mental_shootings <- mass_shootings %>%
  drop_na(prior_mental_illness)

# Want to explore differences in mass shootings with shooters suffering from mental illness vs. not
# Focus: total victims
mental_plot <- ggplot(mental_shootings, aes(x = total_victims, fill = prior_mental_illness)) +
  geom_density(alpha = 0.6) +
  labs(x = 'Total Victims', y = 'Frequency', title = 'Distribution of Total Victims of Mass Shootings') +
  guides(fill = guide_legend(title = 'Prior Mental Illness')) + 
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
mental_plot
# No history of mental illness trends to lower numbers of total victims

```

-   Assess the relationship between mental illness and total victims, mental illness and location type, and the intersection of all three variables.

```{r}
# Mental illness & total victims
mv_plot <- ggplot(mental_shootings, aes(x = prior_mental_illness, y = total_victims)) +
  geom_boxplot(outlier.shape = NA, aes(fill = prior_mental_illness), show.legend = FALSE) +
  coord_cartesian(ylim = c(0, 50)) + # Taking out outliers and readjusting axes
  labs(x = 'Prior Mental Illness', y = 'Total Victims', title = 'Number of Total Victims vs. Prior Mental Illness') +
  scale_fill_brewer(palette = 'Dark') +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
mv_plot

# I would point out that the fact that there are a lot of instances with no information on the prior mental illness bucket does take away many observations that we're not using to measure the relationship.

```

```{r}
# Mental illness & location type
mental_location <- mental_shootings %>%
  group_by(location_type, prior_mental_illness) %>%
  summarise(number = n())

mental_plot3 <- ggplot(mental_location, aes(x = location_type, y = prior_mental_illness)) + 
  geom_point(aes(size = number)) +
  labs(y = 'Prior Mental Illness', x = 'Location Type', title = 'Location Type vs. Prior Mental Illness', size = 'No. Shootings') +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
mental_plot3

# The presence of the 'Other' option in location type throws off the data because it is not useful in identifying trends in location type to prior mental illness. We can potentially say that having a prior mental illness makes it more likely for the shooter to be more random in how they attack.
```

```{r}
# Mental illness, location type & total victims
mental_location <- mental_shootings %>%
  group_by(location_type, prior_mental_illness) %>%
  summarise(victims = sum(total_victims))

mental_plot4 <- ggplot(mental_location, aes(x = location_type, y = prior_mental_illness, fill = victims)) + 
  geom_tile() +
  labs(y = 'Prior Mental Illness', x = 'Location Type', title = 'Location Type vs. Prior Mental Illness') +
  guides(fill = guide_colourbar(title = "Total Victims")) +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
mental_plot4

# There are definitely less instances of mass shootings with no prior mental illness. It is quite difficult to identify significant relationships between these variables (location type and prior mental illness) because they are discrete.


```


Make sure to provide a couple of sentences of written interpretation of your tables/figures. Graphs and tables alone will not be sufficient to answer this question.

# Exploring credit card fraud

We will be using a dataset with credit card transactions containing legitimate and fraud transactions. Fraud is typically well below 1% of all transactions, so a naive model that predicts that all transactions are legitimate and not fraudulent would have an accuracy of well over 99%-- pretty good, no? (well, not quite as we will see later in the course)

You can read more on credit card fraud on [Credit Card Fraud Detection Using Weighted Support Vector Machine](https://www.scirp.org/journal/paperinformation.aspx?paperid=105944)

The dataset we will use consists of credit card transactions and it includes information about each transaction including customer details, the merchant and category of purchase, and whether or not the transaction was a fraud.

## Obtain the data

The dataset is too large to be hosted on Canvas or Github, so please download it from dropbox https://www.dropbox.com/sh/q1yk8mmnbbrzavl/AAAxzRtIhag9Nc_hODafGV2ka?dl=0 and save it in your `dsb` repo, under the `data` folder

```{r}
#| echo: false
#| message: false
#| warning: false

card_fraud <- read_csv(here::here("data", "card_fraud.csv"))

glimpse(card_fraud)
```

The data dictionary is as follows

| column(variable)      | description                                 |
|-----------------------|---------------------------------------------|
| trans_date_trans_time | Transaction DateTime                        |
| trans_year            | Transaction year                            |
| category              | category of merchant                        |
| amt                   | amount of transaction                       |
| city                  | City of card holder                         |
| state                 | State of card holder                        |
| lat                   | Latitude location of purchase               |
| long                  | Longitude location of purchase              |
| city_pop              | card holder's city population               |
| job                   | job of card holder                          |
| dob                   | date of birth of card holder                |
| merch_lat             | Latitude Location of Merchant               |
| merch_long            | Longitude Location of Merchant              |
| is_fraud              | Whether Transaction is Fraud (1) or Not (0) |

-   In this dataset, how likely are fraudulent transactions? Generate a table that summarizes the number and frequency of fraudulent transactions per year.

```{r}
# Data frame that will group by year and whether it was fraud or not
transaction_year <- card_fraud %>%
  group_by(trans_year, is_fraud) %>%
  summarise(frequency = n()) 
  
# Data frame that will group by year 
total_transactions <- card_fraud %>%
  group_by(trans_year) %>%
  summarise(frequency = n())

# Table that summarises likelihoods of fraud across years
fraud_probability <- data.frame(
  fraud = c('Yes', 'No'),
  yr19 = c(round(transaction_year[[2, 3]] / total_transactions[[1, 2]], 3), 
                round(transaction_year[[1, 3]] / total_transactions[[1, 2]], 3)),
  yr20 = c(round(transaction_year[[4, 3]] / total_transactions[[2, 2]], 3), 
                round(transaction_year[[3, 3]] / total_transactions[[2, 2]], 3))
)
  
head(fraud_probability, 2)
# The likelihood for fraud was consistent across 2019 and 2020

```

-   How much money (in US\$ terms) are fraudulent transactions costing the company? Generate a table that summarizes the total amount of legitimate and fraudulent transactions per year and calculate the % of fraudulent transactions, in US\$ terms.

```{r}
# Transaction amounts per year
total_amount <- card_fraud %>%
  group_by(trans_year) %>%
  summarise(amount = sum(amt)) 

# Transaction amounts per year and by whether or not it was fraud
cost_transactions <- card_fraud %>%
  group_by(trans_year, is_fraud) %>%
  summarise(amount = round(sum(amt))) 

# Calculating the % of transactions 
percent <- c(round(cost_transactions[[1, 3]] / total_amount[[1, 2]], 3), 
             round(cost_transactions[[2, 3]] / total_amount[[1, 2]], 3),
             round(cost_transactions[[3, 3]] / total_amount[[2, 2]], 3),
             round(cost_transactions[[4, 3]] / total_amount[[2, 2]], 3))

# Adding the % column back to the cost data frame
cost_transactions$percent <- percent

head(cost_transactions, 4)

```

-   Generate a histogram that shows the distribution of amounts charged to credit card, both for legitimate and fraudulent accounts. Also, for both types of transactions, calculate some quick summary statistics.

```{r}
card_transactions <- card_fraud %>%
  mutate(is_fraud = as.factor(is_fraud))

cost_histogram <- ggplot(card_transactions, aes(x = amt, fill = is_fraud)) +
  geom_histogram(color = 'white', bins = 50, position = 'identity', alpha = 0.8) +
  scale_x_log10() +
  labs(x = 'Transaction Amounts', y = 'Frequency', title = 'Distribution of Card Transaction Amounts (Log Scale)') +
  scale_fill_discrete(name = 'Fraud', labels = c('No', 'Yes')) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

cost_histogram

```

```{r}
# Summary statistics for fraudulent transactions
fraud_summary <- card_fraud %>%
  filter(is_fraud == 1) %>%
  summarise(Transactions = n(),
            Amounts = round(sum(amt)),
            Average = round(mean(amt)),
            Median = round(median(amt)),
            STD = round(sd(amt)), 
            Max = round(max(amt)),
            Min = round(min(amt))
            ) %>%
  pivot_longer(everything(), names_to = 'Metrics', values_to = 'Fraud')

nofraud_summary <- card_fraud %>%
  filter(is_fraud == 0) %>%
  summarise(Transactions = n(),
            Amounts = round(sum(amt)),
            Average = round(mean(amt)),
            Median = round(median(amt)),
            STD = round(sd(amt)), 
            Max = round(max(amt)),
            Min = round(min(amt))
            ) %>%
  pivot_longer(everything(), names_to = 'Metrics', values_to = 'No_Fraud')

summary_transactions <- fraud_summary %>%
  left_join(nofraud_summary, by = 'Metrics')

summary_transactions
# From these summary transactions, the data suggests that fraudulent transactions are likely to be bigger ticket items (on average), as evidenced by the mean/median statistics.
```

-   What types of purchases are most likely to be instances of fraud? Consider category of merchants and produce a bar chart that shows % of total fraudulent transactions sorted in order.

```{r}
total_fraud = card_fraud %>%
  filter(is_fraud == 1) %>%
  summarise(total = n())
no_fraud <- total_fraud[[1, 1]]

# Fraud transactions by category, adding column to calculate % of total
category_fraud <- card_fraud %>%
  filter(is_fraud == 1) %>%
  group_by(category) %>%
  summarise(number = n()) %>%
  mutate(percent = round(number / no_fraud, 2))

# Bar chart to show % of total fradulent transactions 
category_chart <- ggplot(category_fraud, aes(x = fct_reorder(category, number), y = number)) +
  geom_col(fill = 'darkorange') + 
  geom_text(aes(label = percent), hjust = 1.5, size = 3) +
  coord_flip() +
  labs(x = 'Category', y = 'Transactions', title = 'Fraudulent Transactions by Category') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
category_chart


```

-   When is fraud more prevalent? Which days, months, hours? To create new variables to help you in your analysis, we use the `lubridate` package and the following code

```{r}
card_time <- card_fraud %>%
  mutate(
    date_only = lubridate::date(trans_date_trans_time),
    month_name = lubridate::month(trans_date_trans_time, label=TRUE),
    hour = lubridate::hour(trans_date_trans_time),
    weekday = lubridate::wday(trans_date_trans_time, label = TRUE)
    )

# Summarising by month
months_fraud <- card_time %>%
  filter(is_fraud == 1) %>%
  group_by(month_name) %>%
  summarise(number = n()) %>%
  mutate(percent = round(number / no_fraud, 2)) %>%
  arrange(desc(number))
  
months_chart <- ggplot(months_fraud, aes(x = fct_reorder(month_name, number), y = number)) +
  geom_col(fill = 'brown3') + 
  geom_text(aes(label = percent), vjust = 1.5, size = 3) +
  labs(x = 'Month', y = 'Transactions', title = 'Fraudulent Transactions by Month') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
months_chart

# The months with highest proportion of fraudulent transactions are May and March. The ones with the lowest proportion of fraudulent transactions are July and August.

```

```{r}
# Summarising by days
days_fraud <- card_time %>%
  filter(is_fraud == 1) %>%
  group_by(weekday) %>%
  summarise(number = n()) %>%
  mutate(percent = round(number / no_fraud, 2)) %>%
  arrange(desc(number))
  
days_chart <- ggplot(days_fraud, aes(x = fct_reorder(weekday, number), y = number)) +
  geom_col(fill = 'darkgreen') + 
  geom_text(aes(label = percent), vjust = 1.5, size = 3) +
  labs(x = 'Weekday', y = 'Transactions', title = 'Fraudulent Transactions by Weekday') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
days_chart

# The days with highest proportion of fraudulent transactions are Saturday and Monday. The ones with the lowest proportion of fraudulent transactions are Wednesday and Tuesday.

```

```{r}
# Summarising by hours
hours_fraud <- card_time %>%
  filter(is_fraud == 1) %>%
  group_by(hour) %>%
  summarise(number = n()) %>%
  mutate(percent = round(number / no_fraud, 2)) %>%
  arrange(desc(number))
  
hours_chart <- ggplot(hours_fraud, aes(x = hour, y = number)) +
  geom_col(fill = 'sienna3') + 
  labs(x = 'Hour', y = 'Transactions', title = 'Fraudulent Transactions by Hour') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
hours_chart

# The hours with highest proportion of fraudulent transactions are 23 and 22. The ones with the lowest proportion of fraudulent transactions are 6 and 11.

```

-   Are older customers significantly more likely to be victims of credit card fraud? To calculate a customer's age, we use the `lubridate` package and the following code


```{r}
card_age <- card_time %>%
  mutate(age = interval(dob, trans_date_trans_time) / years(1),)

age_fraud <- card_age %>%
  filter(is_fraud == 1)

# Visualising age distribution through a histogram
age_histogram <- ggplot(age_fraud, aes(x = age)) +
  geom_histogram(color = 'white', fill = 'coral3', bins = 50, position = 'identity', alpha = 0.8) +
  labs(x = 'Age', y = 'Frequency', title = 'Distribution of Age in Fraud') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

age_histogram
  
# Fraud is less prevalent in younger and older people

```

-   Is fraud related to distance? The distance between a card holder's home and the location of the transaction can be a feature that is related to fraud. To calculate distance, we need the latidue/longitude of card holders's home and the latitude/longitude of the transaction, and we will use the [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula) to calculate distance. I adapted code to [calculate distance between two points on earth](https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/) which you can find below

```{r}
# distance between card holder's home and transaction
# code adapted from https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/


card_fraud <- card_fraud %>%
  mutate( # convert latitude/longitude to radians
    lat1_radians = lat / 57.29577951,
    lat2_radians = merch_lat / 57.29577951,
    long1_radians = long / 57.29577951,
    long2_radians = merch_long / 57.29577951,
    # calculate distance in miles
    distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)),
    # calculate distance in km
    distance_km = 6377.830272 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians))
  )

```

Plot a boxplot or a violin plot that looks at the relationship of distance and `is_fraud`. Does distance seem to be a useful feature in explaining fraud?

```{r}
# Boxplot on fraud vs. distance
distance_fraud <- card_fraud %>%
  mutate(fraud = case_when(is_fraud == 1 ~ 'Fraud',
                           is_fraud == 0 ~ 'Not Fraud'))

distance_plot <- ggplot(distance_fraud, aes(x = fraud, y = distance_km)) +
  geom_boxplot(aes(fill = fraud), show.legend = FALSE) +
  labs(x = 'Fraud', y = 'Distance (KM)', title = 'Distance vs. Fraud') +
  scale_fill_brewer() +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
distance_plot

```


# Exploring sources of electricity production, CO2 emissions, and GDP per capita.

There are many sources of data on how countries generate their electricity and their CO2 emissions. I would like you to create three graphs:

## 1. A stacked area chart that shows how your own country generated its electricity since 2000.

You will use

`geom_area(colour="grey90", alpha = 0.5, position = "fill")`

## 2. A scatter plot that looks at how CO2 per capita and GDP per capita are related

## 3. A scatter plot that looks at how electricity usage (kWh) per capita/day GDP per capita are related

We will get energy data from the Our World in Data website, and CO2 and GDP per capita emissions from the World Bank, using the `wbstats`package.

```{r}
#| message: false
#| warning: false

# Download electricity data
url <- "https://nyc3.digitaloceanspaces.com/owid-public/data/energy/owid-energy-data.csv"

energy <- read_csv(url) %>% 
  filter(year >= 1990) %>% 
  drop_na(iso_code) %>% 
  select(1:3,
         biofuel = biofuel_electricity,
         coal = coal_electricity,
         gas = gas_electricity,
         hydro = hydro_electricity,
         nuclear = nuclear_electricity,
         oil = oil_electricity,
         other_renewable = other_renewable_exc_biofuel_electricity,
         solar = solar_electricity,
         wind = wind_electricity, 
         electricity_demand,
         electricity_generation,
         net_elec_imports,	# Net electricity imports, measured in terawatt-hours
         energy_per_capita,	# Primary energy consumption per capita, measured in kilowatt-hours	Calculated by Our World in Data based on BP Statistical Review of World Energy and EIA International Energy Data
         energy_per_gdp,	# Energy consumption per unit of GDP. This is measured in kilowatt-hours per 2011 international-$.
         per_capita_electricity, #	Electricity generation per capita, measured in kilowatt-hours
  ) 

# Download data for C02 emissions per capita https://data.worldbank.org/indicator/EN.ATM.CO2E.PC
co2_percap <- wb_data(country = "countries_only", 
                      indicator = "EN.ATM.CO2E.PC", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         co2percap = value)


# Download data for GDP per capita  https://data.worldbank.org/indicator/NY.GDP.PCAP.PP.KD
gdp_percap <- wb_data(country = "countries_only", 
                      indicator = "NY.GDP.PCAP.PP.KD", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         GDPpercap = value) 
```

Specific questions:

1.  How would you turn `energy` to long, tidy format?
2.  You may need to join these data frames
    -   Use `left_join` from `dplyr` to [join the tables](http://r4ds.had.co.nz/relational-data.html)
    -   To complete the merge, you need a unique *key* to match observations between the data frames. Country names may not be consistent among the three dataframes, so please use the 3-digit ISO code for each country
    -   An aside: There is a great package called [`countrycode`](https://github.com/vincentarelbundock/countrycode) that helps solve the problem of inconsistent country names (Is it UK? United Kingdom? Great Britain?). `countrycode()` takes as an input a country's name in a specific format and outputs it using whatever format you specify.
3.  Write a function that takes as input any country's name and returns all three graphs. You can use the `patchwork` package to arrange the three graphs as shown below

```{r}
# Joining the three data sets on countries
energy_data <- energy %>%
  left_join(gdp_percap, by = c('iso_code' = 'iso3c', 'year')) %>%
  left_join(co2_percap, by = c('iso_code' = 'iso3c', 'year'))

```


```{r}
energy_function <- function(country_code, data) {
  country_data <- data %>%
    filter(iso_code == country_code)
  
  ed <- country_data %>%
    select(year, biofuel, coal, gas, hydro, nuclear, oil, solar, wind, electricity_generation) %>%
    pivot_longer(!c(electricity_generation, year), names_to = 'Source', values_to = 'Percentage') %>%
    drop_na()
  
  p1 <- ggplot(ed, aes(x = year, y = Percentage, fill = Source)) + 
    geom_area(position = 'fill', color = 'white') + 
    scale_fill_brewer(palette = 'Blues') + 
    labs(x = 'Year', y = 'Percentage', title = 'Electricity Production Mix') +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))
  
  cd <- country_data %>%
    select(year, co2percap, GDPpercap) %>%
    drop_na()

  
  p2 <- ggplot(cd, aes(x = GDPpercap, y = co2percap)) + 
    geom_point() + 
    geom_label_repel(aes(label = year), size = 2, max.overlaps = Inf) +
    labs(x = 'GDP Per Capita ($)', y = 'CO2 Per Capita', title = 'CO2 vs. GDP Per Capita') +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))
  
  cd2 <- country_data %>%
    select(year, co2percap, energy_per_capita) %>%
    drop_na()

  
  p3 <- ggplot(cd2, aes(x = energy_per_capita, y = co2percap)) + 
    geom_point() + 
    geom_label_repel(aes(label = year), size = 2, max.overlaps = Inf) +
    labs(x = 'Energy Per Capita (kWh)', y = 'CO2 Per Capita', title = 'CO2 vs. Energy Per Capita') +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))
  
  plot <- (p1 / (p2 + p3)) + plot_annotation(title = 'Mexico: Energy Data')

  return(plot)
}

```


```{r}
# Plot for Mexico
plot <- energy_function('MEX', energy_data)
plot

```




=======
---
title: "Homework 2"
author: "Andrea Villarreal"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---
---

```{r}
#| label: Loading Libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(wbstats)
library(skimr)
library(countrycode)
library(here)
library(ggrepel)
library(patchwork)
```

# Data Visualisation - Exploration

Now that you've demonstrated your software is setup, and you have the basics of data manipulation, the goal of this assignment is to practice transforming, visualising, and exploring data.

# Mass shootings in the US

In July 2012, in the aftermath of a mass shooting in a movie theater in Aurora, Colorado, [Mother Jones](https://www.motherjones.com/politics/2012/07/mass-shootings-map/) published a report on mass shootings in the United States since 1982. Importantly, they provided the underlying data set as [an open-source database](https://www.motherjones.com/politics/2012/12/mass-shootings-mother-jones-full-data/) for anyone interested in studying and understanding this criminal behavior.

## Obtain the data

```{r}
#| echo: false
#| message: false
#| warning: false


mass_shootings <- read_csv(here::here("data", "mass_shootings.csv"))

glimpse(mass_shootings)
```

| column(variable)     | description                                                                 |
|--------------------------|----------------------------------------------|
| case                 | short name of incident                                                      |
| year, month, day     | year, month, day in which the shooting occurred                             |
| location             | city and state where the shooting occcurred                                 |
| summary              | brief description of the incident                                           |
| fatalities           | Number of fatalities in the incident, excluding the shooter                 |
| injured              | Number of injured, non-fatal victims in the incident, excluding the shooter |
| total_victims        | number of total victims in the incident, excluding the shooter              |
| location_type        | generic location in which the shooting took place                           |
| male                 | logical value, indicating whether the shooter was male                      |
| age_of_shooter       | age of the shooter when the incident occured                                |
| race                 | race of the shooter                                                         |
| prior_mental_illness | did the shooter show evidence of mental illness prior to the incident?      |

## Explore the data

### Specific questions

-   Generate a data frame that summarizes the number of mass shootings per year.

```{r}

# Data frame groups by year and new column is created with frequency per year
yearly_shootings <- mass_shootings %>%
  group_by(year) %>%
  summarise(number = n(), fatalities = sum(fatalities), injured = sum(injured), total_victims = sum(total_victims))

# Plotting a bar chart with number of shootings each year
year_plot <- ggplot(yearly_shootings) + 
  geom_col(aes(y = number, x = year, fill = year), show.legend = FALSE) + 
  labs(x = 'Year', y = 'Number of Shootings', title = 'Number of Mass Shootings per Year') +
  scale_fill_fermenter() +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
year_plot

```

-   Generate a bar chart that identifies the number of mass shooters associated with each race category. The bars should be sorted from highest to lowest and each bar should show its number.

```{r}
# Data frame that groups by race
race_shootings <- mass_shootings %>%
  group_by(race) %>%
  summarise(sum = n()) %>%
  drop_na() %>%
  arrange(desc(sum))

# Plotting a bar chart with number of shootings per race
race_plot <- ggplot(race_shootings) +
  geom_col(aes(x = reorder(race, sum), y = sum, fill = race), show.legend = FALSE) +
  geom_text(aes(x = reorder(race, sum), y = sum, label = sum), vjust = -0.5, size = 3) +
  labs(x = 'Race', y = 'Number of Shootings', title = 'Number of Mass Shootings per Race') +
  scale_fill_brewer() +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
race_plot

```

-   Generate a boxplot visualizing the number of total victims, by type of location.

```{r}
# To visualise the number of total victims by type of location, there is no need to create a new dataframe that summarises the data

# Plotting a boxplot with the number of total victims by location type
location_plot <- ggplot(mass_shootings, aes(x = location_type, y = total_victims)) +
  geom_boxplot(aes(fill = location_type), show.legend = FALSE) + 
  labs(x = 'Location Type', y = 'Total Victims', title = 'Number of Total Victims by Location Type') +
  scale_fill_brewer(palette = 'Dark') +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
location_plot
```

-   Redraw the same plot, but remove the Las Vegas Strip massacre from the dataset.

```{r}
# To visualise the number of total victims by type of location, there is no need to create a new dataframe that summarises the data

# Plotting a boxplot with the number of total victims by location type
location_plot2 <- ggplot(mass_shootings, aes(x = location_type, y = total_victims)) +
  geom_boxplot(outlier.shape = NA, aes(fill = location_type), show.legend = FALSE) + 
  coord_cartesian(ylim = c(0, 50)) + # Taking out outliers and readjusting axes
  labs(x = 'Location Type', y = 'Total Victims', title = 'Number of Total Victims by Location Type') +
  scale_fill_brewer(palette = 'Dark') +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
location_plot2
```

### More open-ended questions

Address the following questions. Generate appropriate figures/tables to support your conclusions.

-   How many white males with prior signs of mental illness initiated a mass shooting after 2000?

```{r}
# Data frame to filter for the key criteria (race, year and gender)
white_shooters <- mass_shootings %>%
  filter(race == 'White' & year > 2000 & male == TRUE) %>%
  group_by(prior_mental_illness) %>%
  summarise(number = n())

head(white_shooters, 3)

```

-   Which month of the year has the most mass shootings? Generate a bar chart sorted in chronological (natural) order (Jan-Feb-Mar- etc) to provide evidence of your answer.

```{r}
# Data frame to summarize shootings by month
month_shootings <- mass_shootings %>%
  group_by(month) %>%
  summarise(number = n()) %>%
  mutate(month = factor(month, levels = month.abb)) %>%
  arrange(month)

# Plotting a bar graph with shootings by month
month_plot <- ggplot(month_shootings, aes(x = month, y = number)) +
  geom_col(aes(fill = month), show.legend = FALSE) + 
  labs(x = 'Month', y = 'Number of Mass Shootings', title = 'Number of Mass Shootings by Month') +
  scale_fill_brewer(palette = 'Set3') +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
month_plot
```

-   How does the distribution of mass shooting fatalities differ between White and Black shooters? What about White and Latino shooters?

```{r}
# Data frame that filters data for 'black' and 'white' shooters
bw_shootings <- mass_shootings %>%
  filter(race == 'White' | race == 'Black') %>%
  filter(total_victims < 150) # Excluding the Las Vegas shooting

# Histogram for distribution of total victims - Black vs. White
bw_hist <- ggplot(bw_shootings, aes(x = total_victims, fill = race)) +
  geom_density(alpha = 0.6) +
  labs(x = 'Total Victims', y = 'Frequency', title = 'Distribution of Total Victims of Mass Shootings') +
  guides(fill = guide_legend(title = 'Race')) +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))  
bw_hist
```

```{r}
# Data frame that filters data for 'latino' and 'white' shooters
lw_shootings <- mass_shootings %>%
  filter(race == 'White' | race == 'Latino') %>%
  filter(total_victims < 150) # Excluding the Las Vegas shooting

# Histogram for distribution of total victims - Latino vs. White
lw_hist <- ggplot(lw_shootings, aes(x = total_victims, fill = race)) +
  geom_density(alpha = 0.6) +
  labs(x = 'Total Victims', y = 'Frequency', title = 'Distribution of Total Victims of Mass Shootings') +
  guides(fill = guide_legend(title = 'Race')) +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))  
lw_hist
```

### Very open-ended

-   Are mass shootings with shooters suffering from mental illness different from mass shootings with no signs of mental illness in the shooter?

```{r}
# Data frame that filters data for mental illness (excludes NA)
mental_shootings <- mass_shootings %>%
  drop_na(prior_mental_illness)

# Want to explore differences in mass shootings with shooters suffering from mental illness vs. not
# Focus: total victims
mental_plot <- ggplot(mental_shootings, aes(x = total_victims, fill = prior_mental_illness)) +
  geom_density(alpha = 0.6) +
  labs(x = 'Total Victims', y = 'Frequency', title = 'Distribution of Total Victims of Mass Shootings') +
  guides(fill = guide_legend(title = 'Prior Mental Illness')) + 
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
mental_plot
# No history of mental illness trends to lower numbers of total victims

```

-   Assess the relationship between mental illness and total victims, mental illness and location type, and the intersection of all three variables.

```{r}
# Mental illness & total victims
mv_plot <- ggplot(mental_shootings, aes(x = prior_mental_illness, y = total_victims)) +
  geom_boxplot(outlier.shape = NA, aes(fill = prior_mental_illness), show.legend = FALSE) +
  coord_cartesian(ylim = c(0, 50)) + # Taking out outliers and readjusting axes
  labs(x = 'Prior Mental Illness', y = 'Total Victims', title = 'Number of Total Victims vs. Prior Mental Illness') +
  scale_fill_brewer(palette = 'Dark') +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
mv_plot

# I would point out that the fact that there are a lot of instances with no information on the prior mental illness bucket does take away many observations that we're not using to measure the relationship.

```

```{r}
# Mental illness & location type
mental_location <- mental_shootings %>%
  group_by(location_type, prior_mental_illness) %>%
  summarise(number = n())

mental_plot3 <- ggplot(mental_location, aes(x = location_type, y = prior_mental_illness)) + 
  geom_point(aes(size = number)) +
  labs(y = 'Prior Mental Illness', x = 'Location Type', title = 'Location Type vs. Prior Mental Illness', size = 'No. Shootings') +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
mental_plot3

# The presence of the 'Other' option in location type throws off the data because it is not useful in identifying trends in location type to prior mental illness. We can potentially say that having a prior mental illness makes it more likely for the shooter to be more random in how they attack.
```

```{r}
# Mental illness, location type & total victims
mental_location <- mental_shootings %>%
  group_by(location_type, prior_mental_illness) %>%
  summarise(victims = sum(total_victims))

mental_plot4 <- ggplot(mental_location, aes(x = location_type, y = prior_mental_illness, fill = victims)) + 
  geom_tile() +
  labs(y = 'Prior Mental Illness', x = 'Location Type', title = 'Location Type vs. Prior Mental Illness') +
  guides(fill = guide_colourbar(title = "Total Victims")) +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
mental_plot4

# There are definitely less instances of mass shootings with no prior mental illness. It is quite difficult to identify significant relationships between these variables (location type and prior mental illness) because they are discrete.


```


Make sure to provide a couple of sentences of written interpretation of your tables/figures. Graphs and tables alone will not be sufficient to answer this question.

# Exploring credit card fraud

We will be using a dataset with credit card transactions containing legitimate and fraud transactions. Fraud is typically well below 1% of all transactions, so a naive model that predicts that all transactions are legitimate and not fraudulent would have an accuracy of well over 99%-- pretty good, no? (well, not quite as we will see later in the course)

You can read more on credit card fraud on [Credit Card Fraud Detection Using Weighted Support Vector Machine](https://www.scirp.org/journal/paperinformation.aspx?paperid=105944)

The dataset we will use consists of credit card transactions and it includes information about each transaction including customer details, the merchant and category of purchase, and whether or not the transaction was a fraud.

## Obtain the data

The dataset is too large to be hosted on Canvas or Github, so please download it from dropbox https://www.dropbox.com/sh/q1yk8mmnbbrzavl/AAAxzRtIhag9Nc_hODafGV2ka?dl=0 and save it in your `dsb` repo, under the `data` folder

```{r}
#| echo: false
#| message: false
#| warning: false

card_fraud <- read_csv(here::here("data", "card_fraud.csv"))

glimpse(card_fraud)
```

The data dictionary is as follows

| column(variable)      | description                                 |
|-----------------------|---------------------------------------------|
| trans_date_trans_time | Transaction DateTime                        |
| trans_year            | Transaction year                            |
| category              | category of merchant                        |
| amt                   | amount of transaction                       |
| city                  | City of card holder                         |
| state                 | State of card holder                        |
| lat                   | Latitude location of purchase               |
| long                  | Longitude location of purchase              |
| city_pop              | card holder's city population               |
| job                   | job of card holder                          |
| dob                   | date of birth of card holder                |
| merch_lat             | Latitude Location of Merchant               |
| merch_long            | Longitude Location of Merchant              |
| is_fraud              | Whether Transaction is Fraud (1) or Not (0) |

-   In this dataset, how likely are fraudulent transactions? Generate a table that summarizes the number and frequency of fraudulent transactions per year.

```{r}
# Data frame that will group by year and whether it was fraud or not
transaction_year <- card_fraud %>%
  group_by(trans_year, is_fraud) %>%
  summarise(frequency = n()) 
  
# Data frame that will group by year 
total_transactions <- card_fraud %>%
  group_by(trans_year) %>%
  summarise(frequency = n())

# Table that summarises likelihoods of fraud across years
fraud_probability <- data.frame(
  fraud = c('Yes', 'No'),
  yr19 = c(round(transaction_year[[2, 3]] / total_transactions[[1, 2]], 3), 
                round(transaction_year[[1, 3]] / total_transactions[[1, 2]], 3)),
  yr20 = c(round(transaction_year[[4, 3]] / total_transactions[[2, 2]], 3), 
                round(transaction_year[[3, 3]] / total_transactions[[2, 2]], 3))
)
  
head(fraud_probability, 2)
# The likelihood for fraud was consistent across 2019 and 2020

```

-   How much money (in US\$ terms) are fraudulent transactions costing the company? Generate a table that summarizes the total amount of legitimate and fraudulent transactions per year and calculate the % of fraudulent transactions, in US\$ terms.

```{r}
# Transaction amounts per year
total_amount <- card_fraud %>%
  group_by(trans_year) %>%
  summarise(amount = sum(amt)) 

# Transaction amounts per year and by whether or not it was fraud
cost_transactions <- card_fraud %>%
  group_by(trans_year, is_fraud) %>%
  summarise(amount = round(sum(amt))) 

# Calculating the % of transactions 
percent <- c(round(cost_transactions[[1, 3]] / total_amount[[1, 2]], 3), 
             round(cost_transactions[[2, 3]] / total_amount[[1, 2]], 3),
             round(cost_transactions[[3, 3]] / total_amount[[2, 2]], 3),
             round(cost_transactions[[4, 3]] / total_amount[[2, 2]], 3))

# Adding the % column back to the cost data frame
cost_transactions$percent <- percent

head(cost_transactions, 4)

```

-   Generate a histogram that shows the distribution of amounts charged to credit card, both for legitimate and fraudulent accounts. Also, for both types of transactions, calculate some quick summary statistics.

```{r}
card_transactions <- card_fraud %>%
  mutate(is_fraud = as.factor(is_fraud))

cost_histogram <- ggplot(card_transactions, aes(x = amt, fill = is_fraud)) +
  geom_histogram(color = 'white', bins = 50, position = 'identity', alpha = 0.8) +
  scale_x_log10() +
  labs(x = 'Transaction Amounts', y = 'Frequency', title = 'Distribution of Card Transaction Amounts (Log Scale)') +
  scale_fill_discrete(name = 'Fraud', labels = c('No', 'Yes')) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

cost_histogram

```

```{r}
# Summary statistics for fraudulent transactions
fraud_summary <- card_fraud %>%
  filter(is_fraud == 1) %>%
  summarise(Transactions = n(),
            Amounts = round(sum(amt)),
            Average = round(mean(amt)),
            Median = round(median(amt)),
            STD = round(sd(amt)), 
            Max = round(max(amt)),
            Min = round(min(amt))
            ) %>%
  pivot_longer(everything(), names_to = 'Metrics', values_to = 'Fraud')

nofraud_summary <- card_fraud %>%
  filter(is_fraud == 0) %>%
  summarise(Transactions = n(),
            Amounts = round(sum(amt)),
            Average = round(mean(amt)),
            Median = round(median(amt)),
            STD = round(sd(amt)), 
            Max = round(max(amt)),
            Min = round(min(amt))
            ) %>%
  pivot_longer(everything(), names_to = 'Metrics', values_to = 'No_Fraud')

summary_transactions <- fraud_summary %>%
  left_join(nofraud_summary, by = 'Metrics')

summary_transactions
# From these summary transactions, the data suggests that fraudulent transactions are likely to be bigger ticket items (on average), as evidenced by the mean/median statistics.
```

-   What types of purchases are most likely to be instances of fraud? Consider category of merchants and produce a bar chart that shows % of total fraudulent transactions sorted in order.

```{r}
total_fraud = card_fraud %>%
  filter(is_fraud == 1) %>%
  summarise(total = n())
no_fraud <- total_fraud[[1, 1]]

# Fraud transactions by category, adding column to calculate % of total
category_fraud <- card_fraud %>%
  filter(is_fraud == 1) %>%
  group_by(category) %>%
  summarise(number = n()) %>%
  mutate(percent = round(number / no_fraud, 2))

# Bar chart to show % of total fradulent transactions 
category_chart <- ggplot(category_fraud, aes(x = fct_reorder(category, number), y = number)) +
  geom_col(fill = 'darkorange') + 
  geom_text(aes(label = percent), hjust = 1.5, size = 3) +
  coord_flip() +
  labs(x = 'Category', y = 'Transactions', title = 'Fraudulent Transactions by Category') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
category_chart


```

-   When is fraud more prevalent? Which days, months, hours? To create new variables to help you in your analysis, we use the `lubridate` package and the following code

```{r}
card_time <- card_fraud %>%
  mutate(
    date_only = lubridate::date(trans_date_trans_time),
    month_name = lubridate::month(trans_date_trans_time, label=TRUE),
    hour = lubridate::hour(trans_date_trans_time),
    weekday = lubridate::wday(trans_date_trans_time, label = TRUE)
    )

# Summarising by month
months_fraud <- card_time %>%
  filter(is_fraud == 1) %>%
  group_by(month_name) %>%
  summarise(number = n()) %>%
  mutate(percent = round(number / no_fraud, 2)) %>%
  arrange(desc(number))
  
months_chart <- ggplot(months_fraud, aes(x = fct_reorder(month_name, number), y = number)) +
  geom_col(fill = 'brown3') + 
  geom_text(aes(label = percent), vjust = 1.5, size = 3) +
  labs(x = 'Month', y = 'Transactions', title = 'Fraudulent Transactions by Month') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
months_chart

# The months with highest proportion of fraudulent transactions are May and March. The ones with the lowest proportion of fraudulent transactions are July and August.

```

```{r}
# Summarising by days
days_fraud <- card_time %>%
  filter(is_fraud == 1) %>%
  group_by(weekday) %>%
  summarise(number = n()) %>%
  mutate(percent = round(number / no_fraud, 2)) %>%
  arrange(desc(number))
  
days_chart <- ggplot(days_fraud, aes(x = fct_reorder(weekday, number), y = number)) +
  geom_col(fill = 'darkgreen') + 
  geom_text(aes(label = percent), vjust = 1.5, size = 3) +
  labs(x = 'Weekday', y = 'Transactions', title = 'Fraudulent Transactions by Weekday') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
days_chart

# The days with highest proportion of fraudulent transactions are Saturday and Monday. The ones with the lowest proportion of fraudulent transactions are Wednesday and Tuesday.

```

```{r}
# Summarising by hours
hours_fraud <- card_time %>%
  filter(is_fraud == 1) %>%
  group_by(hour) %>%
  summarise(number = n()) %>%
  mutate(percent = round(number / no_fraud, 2)) %>%
  arrange(desc(number))
  
hours_chart <- ggplot(hours_fraud, aes(x = hour, y = number)) +
  geom_col(fill = 'sienna3') + 
  labs(x = 'Hour', y = 'Transactions', title = 'Fraudulent Transactions by Hour') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
hours_chart

# The hours with highest proportion of fraudulent transactions are 23 and 22. The ones with the lowest proportion of fraudulent transactions are 6 and 11.

```

-   Are older customers significantly more likely to be victims of credit card fraud? To calculate a customer's age, we use the `lubridate` package and the following code


```{r}
card_age <- card_time %>%
  mutate(age = interval(dob, trans_date_trans_time) / years(1),)

age_fraud <- card_age %>%
  filter(is_fraud == 1)

# Visualising age distribution through a histogram
age_histogram <- ggplot(age_fraud, aes(x = age)) +
  geom_histogram(color = 'white', fill = 'coral3', bins = 50, position = 'identity', alpha = 0.8) +
  labs(x = 'Age', y = 'Frequency', title = 'Distribution of Age in Fraud') +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

age_histogram
  
# Fraud is less prevalent in younger and older people

```

-   Is fraud related to distance? The distance between a card holder's home and the location of the transaction can be a feature that is related to fraud. To calculate distance, we need the latidue/longitude of card holders's home and the latitude/longitude of the transaction, and we will use the [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula) to calculate distance. I adapted code to [calculate distance between two points on earth](https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/) which you can find below

```{r}
# distance between card holder's home and transaction
# code adapted from https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/


card_fraud <- card_fraud %>%
  mutate( # convert latitude/longitude to radians
    lat1_radians = lat / 57.29577951,
    lat2_radians = merch_lat / 57.29577951,
    long1_radians = long / 57.29577951,
    long2_radians = merch_long / 57.29577951,
    # calculate distance in miles
    distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)),
    # calculate distance in km
    distance_km = 6377.830272 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians))
  )

```

Plot a boxplot or a violin plot that looks at the relationship of distance and `is_fraud`. Does distance seem to be a useful feature in explaining fraud?

```{r}
# Boxplot on fraud vs. distance
distance_fraud <- card_fraud %>%
  mutate(fraud = case_when(is_fraud == 1 ~ 'Fraud',
                           is_fraud == 0 ~ 'Not Fraud'))

distance_plot <- ggplot(distance_fraud, aes(x = fraud, y = distance_km)) +
  geom_boxplot(aes(fill = fraud), show.legend = FALSE) +
  labs(x = 'Fraud', y = 'Distance (KM)', title = 'Distance vs. Fraud') +
  scale_fill_brewer() +
  theme_bw() + 
  theme(plot.title = element_text(hjust = 0.5))
distance_plot

```


# Exploring sources of electricity production, CO2 emissions, and GDP per capita.

There are many sources of data on how countries generate their electricity and their CO2 emissions. I would like you to create three graphs:

## 1. A stacked area chart that shows how your own country generated its electricity since 2000.

You will use

`geom_area(colour="grey90", alpha = 0.5, position = "fill")`

## 2. A scatter plot that looks at how CO2 per capita and GDP per capita are related

## 3. A scatter plot that looks at how electricity usage (kWh) per capita/day GDP per capita are related

We will get energy data from the Our World in Data website, and CO2 and GDP per capita emissions from the World Bank, using the `wbstats`package.

```{r}
#| message: false
#| warning: false

# Download electricity data
url <- "https://nyc3.digitaloceanspaces.com/owid-public/data/energy/owid-energy-data.csv"

energy <- read_csv(url) %>% 
  filter(year >= 1990) %>% 
  drop_na(iso_code) %>% 
  select(1:3,
         biofuel = biofuel_electricity,
         coal = coal_electricity,
         gas = gas_electricity,
         hydro = hydro_electricity,
         nuclear = nuclear_electricity,
         oil = oil_electricity,
         other_renewable = other_renewable_exc_biofuel_electricity,
         solar = solar_electricity,
         wind = wind_electricity, 
         electricity_demand,
         electricity_generation,
         net_elec_imports,	# Net electricity imports, measured in terawatt-hours
         energy_per_capita,	# Primary energy consumption per capita, measured in kilowatt-hours	Calculated by Our World in Data based on BP Statistical Review of World Energy and EIA International Energy Data
         energy_per_gdp,	# Energy consumption per unit of GDP. This is measured in kilowatt-hours per 2011 international-$.
         per_capita_electricity, #	Electricity generation per capita, measured in kilowatt-hours
  ) 

# Download data for C02 emissions per capita https://data.worldbank.org/indicator/EN.ATM.CO2E.PC
co2_percap <- wb_data(country = "countries_only", 
                      indicator = "EN.ATM.CO2E.PC", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         co2percap = value)


# Download data for GDP per capita  https://data.worldbank.org/indicator/NY.GDP.PCAP.PP.KD
gdp_percap <- wb_data(country = "countries_only", 
                      indicator = "NY.GDP.PCAP.PP.KD", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         GDPpercap = value) 
```

Specific questions:

1.  How would you turn `energy` to long, tidy format?
2.  You may need to join these data frames
    -   Use `left_join` from `dplyr` to [join the tables](http://r4ds.had.co.nz/relational-data.html)
    -   To complete the merge, you need a unique *key* to match observations between the data frames. Country names may not be consistent among the three dataframes, so please use the 3-digit ISO code for each country
    -   An aside: There is a great package called [`countrycode`](https://github.com/vincentarelbundock/countrycode) that helps solve the problem of inconsistent country names (Is it UK? United Kingdom? Great Britain?). `countrycode()` takes as an input a country's name in a specific format and outputs it using whatever format you specify.
3.  Write a function that takes as input any country's name and returns all three graphs. You can use the `patchwork` package to arrange the three graphs as shown below

```{r}
# Joining the three data sets on countries
energy_data <- energy %>%
  left_join(gdp_percap, by = c('iso_code' = 'iso3c', 'year')) %>%
  left_join(co2_percap, by = c('iso_code' = 'iso3c', 'year'))

```


```{r}
energy_function <- function(country_code, data) {
  country_data <- data %>%
    filter(iso_code == country_code)
  
  ed <- country_data %>%
    select(year, biofuel, coal, gas, hydro, nuclear, oil, solar, wind, electricity_generation) %>%
    pivot_longer(!c(electricity_generation, year), names_to = 'Source', values_to = 'Percentage') %>%
    drop_na()
  
  p1 <- ggplot(ed, aes(x = year, y = Percentage, fill = Source)) + 
    geom_area(position = 'fill', color = 'white') + 
    scale_fill_brewer(palette = 'Blues') + 
    labs(x = 'Year', y = 'Percentage', title = 'Electricity Production Mix') +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))
  
  cd <- country_data %>%
    select(year, co2percap, GDPpercap) %>%
    drop_na()

  
  p2 <- ggplot(cd, aes(x = GDPpercap, y = co2percap)) + 
    geom_point() + 
    geom_label_repel(aes(label = year), size = 2, max.overlaps = Inf) +
    labs(x = 'GDP Per Capita ($)', y = 'CO2 Per Capita', title = 'CO2 vs. GDP Per Capita') +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))
  
  cd2 <- country_data %>%
    select(year, co2percap, energy_per_capita) %>%
    drop_na()

  
  p3 <- ggplot(cd2, aes(x = energy_per_capita, y = co2percap)) + 
    geom_point() + 
    geom_label_repel(aes(label = year), size = 2, max.overlaps = Inf) +
    labs(x = 'Energy Per Capita (kWh)', y = 'CO2 Per Capita', title = 'CO2 vs. Energy Per Capita') +
    theme_bw() +
    theme(plot.title = element_text(hjust = 0.5))
  
  plot <- (p1 / (p2 + p3)) + plot_annotation(title = 'Mexico: Energy Data')

  return(plot)
}

```


```{r}
# Plot for Mexico
plot <- energy_function('MEX', energy_data)
plot

```




>>>>>>> 24dcee6 (Add files via upload)
